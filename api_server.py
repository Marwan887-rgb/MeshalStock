#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø®Ø§Ø¯Ù… API Ù„Ù†Ø¸Ø§Ù… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
ÙŠÙˆÙØ± endpoints Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Ø¨ØªØ§Øª Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
"""

from flask import Flask, jsonify, request
from flask_cors import CORS
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import subprocess
import threading
import uuid
import queue
import time
import os
from datetime import datetime, timedelta
import yfinance as yf
import pandas as pd
import math
import numpy as np
import jwt
from functools import wraps
from dotenv import load_dotenv

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
load_dotenv()

# Supabase client (optional - falls back to CSV if not configured)
try:
    from supabase_client import get_supabase_client, get_stock_data, get_all_symbols
    USE_SUPABASE = bool(os.getenv('SUPABASE_KEY'))
    if USE_SUPABASE:
        print("âœ“ Supabase enabled - using database for faster performance")
except Exception as e:
    USE_SUPABASE = False
    print(f"âš  Supabase not available, using CSV files: {e}")

app = Flask(__name__, static_url_path='', static_folder='.')

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'dev-secret-key-change-in-production')
app.config['JWT_SECRET_KEY'] = os.getenv('JWT_SECRET_KEY', 'dev-jwt-secret-change-in-production')
app.config['ADMIN_PASSWORD'] = os.getenv('ADMIN_PASSWORD', 'admin123')
app.config['TOKEN_EXPIRY_HOURS'] = int(os.getenv('TOKEN_EXPIRY_HOURS', '24'))

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª CORS - ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ø­Ø¯Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§
allowed_origins = os.getenv('ALLOWED_ORIGINS', '*')
if allowed_origins == '*':
    CORS(app)
else:
    origins_list = [origin.strip() for origin in allowed_origins.split(',')]
    CORS(app, origins=origins_list)

# Rate Limiting Ù„Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† DDoS
if os.getenv('RATE_LIMIT_ENABLED', 'True') == 'True':
    limiter = Limiter(
        app=app,
        key_func=get_remote_address,
        default_limits=[f"{os.getenv('RATE_LIMIT_PER_MINUTE', '60')}/minute"]
    )
else:
    limiter = None

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
# Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
print(f"Base Directory: {BASE_DIR}")

# ØªØ®Ø²ÙŠÙ† Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ø§Ù…
jobs = {}
job_outputs = {}

# Cache Ù„Ù„Ù…Ø¤Ø´Ø±Ø§Øª (ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 10 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„ØªØ¬Ù†Ø¨ rate limiting)
market_cache = {
    'data': None,
    'timestamp': None,
    'ttl': 600  # 10 Ø¯Ù‚Ø§Ø¦Ù‚
}

# ========================================
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© ÙˆØ§Ù„Ø£Ù…Ø§Ù†
# ========================================

def generate_token(username):
    """ØªÙˆÙ„ÙŠØ¯ JWT token"""
    payload = {
        'username': username,
        'exp': datetime.utcnow() + timedelta(hours=app.config['TOKEN_EXPIRY_HOURS']),
        'iat': datetime.utcnow()
    }
    token = jwt.encode(payload, app.config['JWT_SECRET_KEY'], algorithm='HS256')
    return token

def verify_token(token):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙˆÙƒÙ†"""
    try:
        payload = jwt.decode(token, app.config['JWT_SECRET_KEY'], algorithms=['HS256'])
        return payload
    except jwt.ExpiredSignatureError:
        return None  # Ø§Ù„ØªÙˆÙƒÙ† Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
    except jwt.InvalidTokenError:
        return None  # Ø§Ù„ØªÙˆÙƒÙ† ØºÙŠØ± ØµØ§Ù„Ø­

def token_required(f):
    """Decorator Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙˆÙƒÙ†"""
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙˆÙƒÙ† ÙÙŠ Ø§Ù„Ù€ headers
        if 'Authorization' in request.headers:
            auth_header = request.headers['Authorization']
            try:
                token = auth_header.split(' ')[1]  # Bearer <token>
            except IndexError:
                return jsonify({'error': 'Token format invalid'}), 401
        
        if not token:
            return jsonify({'error': 'Token is missing'}), 401
        
        payload = verify_token(token)
        if not payload:
            return jsonify({'error': 'Token is invalid or expired'}), 401
        
        return f(*args, **kwargs)
    
    return decorated

# ========================================
# Routes
# ========================================

@app.route('/')
def root():
    """Ø¹Ø±Ø¶ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return app.send_static_file('index.html')

@app.route('/api/health')
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'version': '2.0.0',
        'environment': os.getenv('FLASK_ENV', 'development')
    })

@app.route('/api/auth/login', methods=['POST'])
def login():
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆÙƒÙ†"""
    data = request.get_json()
    
    if not data or 'password' not in data:
        return jsonify({'error': 'Password is required'}), 400
    
    password = data['password']
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±
    if password == app.config['ADMIN_PASSWORD']:
        token = generate_token('admin')
        return jsonify({
            'success': True,
            'token': token,
            'expires_in': app.config['TOKEN_EXPIRY_HOURS'] * 3600  # Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        })
    else:
        return jsonify({'error': 'Invalid password'}), 401

@app.route('/api/auth/verify', methods=['POST'])
@token_required
def verify():
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ØªÙˆÙƒÙ†"""
    return jsonify({'valid': True})


class JobRunner:
    """ÙØ¦Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    
    def __init__(self, job_id, command):
        self.job_id = job_id
        self.command = command
        self.output_queue = queue.Queue()
        self.process = None
        self.status = 'pending'
        self.progress = 0
        self.total = 0
        self.stats = {}
        
    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©"""
        try:
            self.status = 'running'
            jobs[self.job_id] = {
                'status': self.status,
                'progress': 0,
                'total': 0,
                'started_at': datetime.now().isoformat(),
                'stats': {}
            }
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¶Ù…Ø§Ù† Ø¯Ø¹Ù… UTF-8
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'

            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª
            self.process = subprocess.Popen(
                self.command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                encoding='utf-8',
                bufsize=1,
                universal_newlines=True,
                env=env
            )
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            for line in iter(self.process.stdout.readline, ''):
                if line:
                    self.parse_output(line)
                    job_outputs.setdefault(self.job_id, []).append(line.strip())
                    
                    # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
                    jobs[self.job_id].update({
                        'status': self.status,
                        'progress': self.progress,
                        'total': self.total,
                        'stats': self.stats
                    })
            
            self.process.wait()
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            if self.process.returncode == 0:
                self.status = 'completed'
            else:
                self.status = 'failed'
            
            jobs[self.job_id].update({
                'status': self.status,
                'completed_at': datetime.now().isoformat(),
                'return_code': self.process.returncode
            })
            
        except Exception as e:
            self.status = 'error'
            jobs[self.job_id].update({
                'status': self.status,
                'error': str(e),
                'completed_at': datetime.now().isoformat()
            })
    
    def parse_output(self, line):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙ‚Ø¯Ù…"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³Ø·Ø± Ø§Ù„ØªÙ‚Ø¯Ù…
        if 'Ø§Ù„ØªÙ‚Ø¯Ù…:' in line:
            try:
                # Ø§Ù„ØªÙ‚Ø¯Ù…: 5/243 (2.1%)
                parts = line.split('Ø§Ù„ØªÙ‚Ø¯Ù…:')[1].split('(')[0].strip()
                completed, total = parts.split('/')
                self.progress = int(completed.strip())
                self.total = int(total.strip())
            except:
                pass
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        if 'Ø¬Ø¯ÙŠØ¯:' in line:
            try:
                # Ø¬Ø¯ÙŠØ¯: 5, Ù…Ø­Ø¯Ø«: 10, Ù…Ø­Ø¯Ø« Ù…Ø³Ø¨Ù‚Ø§Ù‹: 2, ÙØ´Ù„: 0
                parts = line.split('-')[-1].strip()
                items = parts.split(',')
                for item in items:
                    key, value = item.split(':')
                    key = key.strip()
                    value = int(value.strip())
                    
                    if 'Ø¬Ø¯ÙŠØ¯' in key:
                        self.stats['new'] = value
                    elif 'Ù…Ø­Ø¯Ø«' in key and 'Ù…Ø³Ø¨Ù‚Ø§Ù‹' not in key:
                        self.stats['updated'] = value
                    elif 'Ù…Ø­Ø¯Ø« Ù…Ø³Ø¨Ù‚Ø§Ù‹' in key:
                        self.stats['up_to_date'] = value
                    elif 'ÙØ´Ù„' in key:
                        self.stats['failed'] = value
            except:
                pass


@app.route('/api/fetch/saudi', methods=['POST'])
@token_required
def fetch_saudi():
    """Ø¨Ø¯Ø¡ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"""
    job_id = str(uuid.uuid4())
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø·Ù„Ø¨
    data = request.get_json() or {}
    test_mode = data.get('test', False)
    workers = data.get('workers', 2)  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù…Ø±
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… sys.executable Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù…ÙØ³Ø± Ø¨Ø§ÙŠØ«ÙˆÙ†
    import sys
    command = [sys.executable, '-u', 'fetch_saudi_data.py', '--workers', str(workers)]
    
    if test_mode:
        command.extend(['--test', '--symbols', '1120.SR,2222.SR,7010.SR'])
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙˆØ±Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Race Condition
    jobs[job_id] = {
        'status': 'starting',
        'progress': 0,
        'total': 0,
        'started_at': datetime.now().isoformat(),
        'stats': {}
    }
    
    runner = JobRunner(job_id, command)
    thread = threading.Thread(target=runner.run)
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'job_id': job_id,
        'status': 'started',
        'message': 'Ø¨Ø¯Ø£Øª Ø¹Ù…Ù„ÙŠØ© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©'
    })


@app.route('/api/fetch/us', methods=['POST'])
@token_required
def fetch_us():
    """Ø¨Ø¯Ø¡ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©"""
    job_id = str(uuid.uuid4())
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ø·Ù„Ø¨
    data = request.get_json() or {}
    test_mode = data.get('test', False)
    workers = data.get('workers', 2)  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù…Ø±
    import sys
    command = [sys.executable, '-u', 'fetch_us_data.py', '--workers', str(workers)]
    
    if test_mode:
        command.extend(['--test', '--symbols', 'AAPL,MSFT,GOOGL'])
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙˆØ±Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Race Condition
    jobs[job_id] = {
        'status': 'starting',
        'progress': 0,
        'total': 0,
        'started_at': datetime.now().isoformat(),
        'stats': {}
    }
    
    runner = JobRunner(job_id, command)
    thread = threading.Thread(target=runner.run)
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'job_id': job_id,
        'status': 'started',
        'message': 'Ø¨Ø¯Ø£Øª Ø¹Ù…Ù„ÙŠØ© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©'
    })


@app.route('/api/market-summary', methods=['GET'])
def market_summary():
    """Ø¬Ù„Ø¨ Ù…Ù„Ø®Øµ Ø§Ù„Ø³ÙˆÙ‚ Ù„Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    # Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    tickers = {
        'TASI': '^TASI.SR',
        'DJI': '^DJI',
        'NASDAQ': '^IXIC',
        'SP500': '^GSPC',
        'OIL': 'CL=F',
        'GOLD': 'GC=F',
        'SILVER': 'SI=F',
        'BTC': 'BTC-USD'
    }
    
    results = {}
    
    try:
        print(f"Fetching market summary for: {list(tickers.keys())}")
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        data = yf.download(list(tickers.values()), period="5d", progress=False, auto_adjust=True)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙŠØ³Øª ÙØ§Ø±ØºØ©
        if data.empty:
            print("Error: yfinance returned empty data")
            return jsonify({'error': 'No data returned from yfinance'}), 500

        print("Data fetched successfully")
        
        for name, symbol in tickers.items():
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø³Ù‡Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯
                # Ù…Ù„Ø§Ø­Ø¸Ø©: yfinance ÙŠØ¹ÙŠØ¯ MultiIndex Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø±Ù…Ø²
                if len(tickers) > 1:
                    try:
                        stock_data = data['Close'][symbol]
                    except KeyError:
                        print(f"Symbol {symbol} not found in data columns: {data.columns}")
                        results[name] = {'error': 'Symbol not found'}
                        continue
                else:
                    stock_data = data['Close']
                
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª
                valid_data = stock_data.dropna()
                if len(valid_data) >= 2:
                    current_price = valid_data.iloc[-1]
                    prev_price = valid_data.iloc[-2]
                    
                    change = current_price - prev_price
                    change_percent = (change / prev_price) * 100
                    
                    results[name] = {
                        'price': round(float(current_price), 2),
                        'change': round(float(change), 2),
                        'change_percent': round(float(change_percent), 2),
                        'status': 'up' if change >= 0 else 'down'
                    }
                    print(f"{name}: {current_price}")
                else:
                    print(f"{name}: Insufficient data (len={len(valid_data)})")
                    results[name] = {'error': 'Insufficient data'}
                    
            except Exception as e:
                print(f"Error processing {name}: {e}")
                results[name] = {'error': str(e)}
                
        return jsonify(results)
        
    except Exception as e:
        print(f"Global error in market summary: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/status/<job_id>', methods=['GET'])
@token_required
def get_status(job_id):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ù…Ù‡Ù…Ø©"""
    if job_id not in jobs:
        return jsonify({
            'error': 'Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©'
        }), 404
    
    job_info = jobs[job_id]
    
    # Ø¥Ø¶Ø§ÙØ© Ø¢Ø®Ø± Ø³Ø·ÙˆØ± Ù…Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
    outputs = job_outputs.get(job_id, [])
    recent_output = outputs[-10:] if len(outputs) > 10 else outputs
    
    return jsonify({
        'job_id': job_id,
        **job_info,
        'recent_output': recent_output
    })


@app.route('/api/jobs', methods=['GET'])
@token_required
def list_jobs():
    """Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù…"""
    return jsonify({
        'jobs': jobs
    })


@app.route('/api/symbols/<market>', methods=['GET'])
def get_symbols(market):
    """Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
    try:
        symbols_map = {}
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ
        if market == 'saudi':
            directory = os.path.join(BASE_DIR, 'data_sa')
            try:
                symbols_path = os.path.join(BASE_DIR, 'symbols_sa.txt')
                if os.path.exists(symbols_path):
                    df_symbols = pd.read_csv(symbols_path)
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø±Ù…ÙˆØ²
                    for _, row in df_symbols.iterrows():
                        sym = str(row['Symbol']).strip()
                        name = str(row['NameAr']).strip()
                        symbols_map[sym] = name
            except Exception as e:
                print(f"Error reading symbols_sa.txt: {e}")
                
        elif market == 'us':
            directory = os.path.join(BASE_DIR, 'data_us')
        else:
            return jsonify({'error': 'Invalid market'}), 400
            
        if not os.path.exists(directory):
            return jsonify({'error': f'Directory {directory} not found'}), 404
            
        symbols = []
        for filename in os.listdir(directory):
            if filename.endswith('.csv'):
                # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ .csv
                symbol = filename[:-4]
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                name_ar = symbols_map.get(symbol, "")
                if not name_ar and market == 'saudi':
                     # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¯ÙˆÙ† .SR Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯
                     clean_sym = symbol.replace('.SR', '')
                     name_ar = symbols_map.get(clean_sym, "")
                
                symbols.append({
                    'symbol': symbol,
                    'name': name_ar if name_ar else (symbol if market == 'us' else 'Ø³Ù‡Ù… Ø³Ø¹ÙˆØ¯ÙŠ')
                })
                
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
        symbols.sort(key=lambda x: x['symbol'])
        
        return jsonify({'symbols': symbols})
        
    except Exception as e:
        print(f"Error listing symbols: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/history/<market>/<symbol>', methods=['GET'])
def get_history(market, symbol):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø³Ù‡Ù… Ù…Ø¹ÙŠÙ† (Ø¢Ø®Ø± 6.5 Ø£Ø´Ù‡Ø±)"""
    try:
        if market == 'saudi':
            directory = os.path.join(BASE_DIR, 'data_sa')
        elif market == 'us':
            directory = os.path.join(BASE_DIR, 'data_us')
        else:
            return jsonify({'error': 'Invalid market'}), 400
            
        file_path = os.path.join(directory, f"{symbol}.csv")
        
        if not os.path.exists(file_path):
            return jsonify({'error': 'File not found'}), 404
            
        # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV
        df = pd.read_csv(file_path)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ®
        df['Date'] = pd.to_datetime(df['Date'])
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© (6 Ø£Ø´Ù‡Ø± Ø¨Ø§Ù„Ø¶Ø¨Ø·)
        end_date = df['Date'].max()
        start_date = end_date - pd.DateOffset(months=6)
        
        # ØªØµÙÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)
        filtered_df = df.loc[mask].copy()
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ JSON
        # Ù†Ø­ØªØ§Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ string
        filtered_df['Date'] = filtered_df['Date'].dt.strftime('%Y-%m-%d')
        
        result = filtered_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].to_dict(orient='records')
        
        return jsonify(result)
        
    except Exception as e:
        print(f"Error fetching history for {symbol}: {e}")
        return jsonify({'error': str(e)}), 500



def calculate_levels(df):
    """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¬Ø§Ù† ÙˆÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ Ù„Ù„Ø³Ù‡Ù…"""
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØªØµÙÙŠØ© Ø¢Ø®Ø± 6 Ø£Ø´Ù‡Ø±
        if 'Date' not in df.columns: return None
        df['Date'] = pd.to_datetime(df['Date'])
        
        if df.empty: return None
        
        end_date = df['Date'].max()
        start_date = end_date - pd.DateOffset(months=6)
        df_filtered = df[df['Date'] >= start_date].copy()
        
        if df_filtered.empty: return None
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚Ù„ Ù‚Ø§Ø¹
        min_low = df_filtered['Low'].min()
        min_low_idx = df_filtered['Low'].idxmin()
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙˆÙ„ Ù‚Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ø¹
        peak_high = None
        
        # Ù†Ø£Ø®Ø° Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ø¹
        df_subset = df_filtered.loc[min_low_idx:].reset_index(drop=True)
        
        if len(df_subset) < 3: return None
        
        for i in range(1, len(df_subset) - 1):
            curr = df_subset.iloc[i]['High']
            prev = df_subset.iloc[i-1]['High']
            next_val = df_subset.iloc[i+1]['High']
            
            if curr > prev and curr > next_val:
                peak_high = curr
                break
                
        if peak_high is None or peak_high <= min_low:
            return None
            
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
        levels = []
        
        # Gann (Dynamic)
        sqrt_low = math.sqrt(min_low)
        sqrt_peak = math.sqrt(peak_high)
        delta = sqrt_peak - sqrt_low
        
        levels.append({'type': 'Gann 180', 'value': (sqrt_low + 2*delta)**2})
        levels.append({'type': 'Gann 270', 'value': (sqrt_low + 3*delta)**2})
        levels.append({'type': 'Gann 360', 'value': (sqrt_low + 4*delta)**2})
        
        # Fibo
        fib_range = peak_high - min_low
        levels.append({'type': 'Fibo 100', 'value': peak_high})
        levels.append({'type': 'Fibo 161.8', 'value': min_low + fib_range * 1.618})
        levels.append({'type': 'Fibo 261.8', 'value': min_low + fib_range * 2.618})
        levels.append({'type': 'Fibo 423.6', 'value': min_low + fib_range * 4.236})
        
        return levels
    except Exception as e:
        print(f"Error calculating levels: {e}")
        return None

@app.route('/api/scan/fibo_gann', methods=['GET'])
def scan_fibo_gann():
    """ÙØ­Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ù‡Ù… Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙØ±Øµ (Ø§Ø®ØªØ±Ø§Ù‚ Ø£Ùˆ Ø§Ø±ØªØ¯Ø§Ø¯)"""
    market = request.args.get('market', 'saudi')
    
    if market == 'saudi':
        directory = os.path.join(BASE_DIR, 'data_sa')
        # ØªØ­Ù…ÙŠÙ„ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
        symbols_map = {}
        try:
            symbols_path = os.path.join(BASE_DIR, 'symbols_sa.txt')
            if os.path.exists(symbols_path):
                df_sym = pd.read_csv(symbols_path)
                for _, row in df_sym.iterrows():
                    symbols_map[str(row['Symbol']).strip()] = str(row['NameAr']).strip()
        except: pass
    elif market == 'us':
        directory = os.path.join(BASE_DIR, 'data_us')
        symbols_map = {}
    else:
        return jsonify({'error': 'Invalid market'}), 400
        
    if not os.path.exists(directory):
        return jsonify({'results': []})
        
    results = []
    
    for filename in os.listdir(directory):
        if not filename.endswith('.csv'): continue
        
        symbol = filename[:-4]
        file_path = os.path.join(directory, filename)
        
        try:
            df = pd.read_csv(file_path)
            levels = calculate_levels(df)
            
            if not levels: continue
            
            # ÙØ­Øµ Ø¢Ø®Ø± Ø´Ù…Ø¹Ø©
            last_candle = df.iloc[-1]
            open_p = last_candle['Open']
            close_p = last_candle['Close']
            high_p = last_candle['High']
            low_p = last_candle['Low']
            
            match = False
            match_reason = ""
            match_level = 0
            
            for level in levels:
                val = level['value']
                
                # Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ø§Ù„Ø´Ù…Ø¹Ø© ØªÙ„Ø§Ù…Ø³ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ (Ø§Ù„Ù‡Ø§ÙŠ Ø£Ø¹Ù„Ù‰ ÙˆØ§Ù„Ù„Ùˆ Ø£Ù‚Ù„)
                if low_p <= val <= high_p:
                    
                    # 1. Ø§Ø®ØªØ±Ø§Ù‚ (Breakout): Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ ÙÙˆÙ‚ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ ÙˆØ§Ù„Ø§ÙØªØªØ§Ø­ ØªØ­ØªÙ‡
                    if open_p < val < close_p:
                        match = True
                        match_reason = f"Ø§Ø®ØªØ±Ø§Ù‚ {level['type']}"
                        match_level = val
                        break
                        
                    # 2. Ø§Ø±ØªØ¯Ø§Ø¯ (Bounce): Ù‡Ø¨Ø· Ù„Ù„Ù…Ø³ØªÙˆÙ‰ (Ø§Ù„Ù„Ùˆ Ø£Ù‚Ù„ Ù…Ù†Ù‡) Ù„ÙƒÙ† Ø£ØºÙ„Ù‚ ÙÙˆÙ‚Ù‡
                    # ÙŠÙØ¶Ù„ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø§ÙØªØªØ§Ø­ Ø£ÙŠØ¶Ø§Ù‹ ÙÙˆÙ‚Ù‡ Ù„ÙŠÙƒÙˆÙ† Ø§Ø±ØªØ¯Ø§Ø¯ ÙˆØ§Ø¶Ø­ØŒ Ø£Ùˆ Ù…Ø¬Ø±Ø¯ Ø°ÙŠÙ„
                    elif low_p <= val and close_p > val:
                        match = True
                        match_reason = f"Ø§Ø±ØªØ¯Ø§Ø¯ Ù…Ù† {level['type']}"
                        match_level = val
                        break
            
            if match:
                name = symbols_map.get(symbol, symbol)
                if market == 'saudi':
                    clean_sym = symbol.replace('.SR', '')
                    name = symbols_map.get(clean_sym, name)
                    
                results.append({
                    'symbol': symbol,
                    'name': name,
                    'close': close_p,
                    'reason': match_reason,
                    'level': match_level
                })
                
        except Exception as e:
            continue
            
    return jsonify({'results': results})


@app.route('/api/market-data/<market>', methods=['GET'])
def get_market_data(market):
    """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„"""
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        target_date = request.args.get('date', None)
        
        # Try Supabase first
        if USE_SUPABASE:
            try:
                return get_market_data_from_supabase(market, target_date)
            except Exception as e:
                print(f"Supabase error, falling back to CSV: {e}")
        
        # Fallback to CSV
        if market == 'saudi':
            directory = os.path.join(BASE_DIR, 'data_sa')
            # ØªØ­Ù…ÙŠÙ„ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
            symbols_map = {}
            try:
                symbols_path = os.path.join(BASE_DIR, 'symbols_sa.txt')
                if os.path.exists(symbols_path):
                    df_sym = pd.read_csv(symbols_path)
                    for _, row in df_sym.iterrows():
                        symbols_map[str(row['Symbol']).strip()] = str(row['NameAr']).strip()
            except: pass
        elif market == 'us':
            directory = os.path.join(BASE_DIR, 'data_us')
            symbols_map = {}
        else:
            return jsonify({'error': 'Invalid market'}), 400
            
        if not os.path.exists(directory):
            print(f"Directory not found: {os.path.abspath(directory)}")
            return jsonify({'data': [], 'date': None, 'available_dates': []})
            
        print(f"Reading market data from: {os.path.abspath(directory)}")
        if target_date:
            print(f"Target date requested: {target_date}")
            
        data_list = []
        actual_date = None  # Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        available_dates = set()  # Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…ØªØ§Ø­Ø©
        
        for filename in os.listdir(directory):
            if not filename.endswith('.csv'): continue
            
            symbol = filename[:-4]
            file_path = os.path.join(directory, filename)
            
            try:
                # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
                df = pd.read_csv(file_path)
                
                if len(df) < 2: continue
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
                if 'Date' in df.columns:
                    df['Date'] = pd.to_datetime(df['Date'])
                    df = df.sort_values('Date')
                
                # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…ØªØ§Ø­Ø©
                for d in df['Date']:
                    available_dates.add(d.strftime('%Y-%m-%d'))
                
                # Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯ ØªØ§Ø±ÙŠØ® Ù…Ø¹ÙŠÙ†
                if target_date:
                    target_dt = pd.to_datetime(target_date)
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ ØªØ§Ø±ÙŠØ®
                    df_filtered = df[df['Date'] <= target_dt]
                    if len(df_filtered) == 0:
                        continue  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®
                    last_row = df_filtered.iloc[-1]
                    
                    # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„ØµÙ Ø§Ù„Ø³Ø§Ø¨Ù‚
                    last_idx = df_filtered.index[-1]
                    if last_idx > 0:
                        prev_row = df.iloc[last_idx - 1]
                    else:
                        prev_row = last_row  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø§Ø¨Ù‚
                else:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ø®Ø± ØªØ§Ø±ÙŠØ®
                    last_row = df.iloc[-1]
                    prev_row = df.iloc[-2]
                
                # Ø­ÙØ¸ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ù„ÙŠ
                if actual_date is None:
                    actual_date = last_row['Date'].strftime('%Y-%m-%d')
                
                price = float(last_row['Close'])
                prev_close = float(prev_row['Close'])
                change = price - prev_close
                change_pct = (change / prev_close) * 100
                volume = int(last_row['Volume'])
                
                name = symbols_map.get(symbol, symbol)
                if market == 'saudi':
                    clean_sym = symbol.replace('.SR', '')
                    name = symbols_map.get(clean_sym, name)
                
                data_list.append({
                    'symbol': symbol,
                    'name': name,
                    'price': round(price, 2),
                    'change': round(change, 2),
                    'change_percent': round(change_pct, 2),
                    'volume': volume
                })
                
            except Exception as e:
                continue
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø¯Ø¯
        if target_date and len(data_list) == 0:
            return jsonify({
                'data': [],
                'date': target_date,
                'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®'
            })
                
        return jsonify({
            'data': data_list,
            'date': actual_date,
            'available_dates': sorted(list(available_dates), reverse=True)[:30]  # Ø¢Ø®Ø± 30 ØªØ§Ø±ÙŠØ®
        })
        
    except Exception as e:
        print(f"Error getting market data: {e}")
        return jsonify({'error': str(e)}), 500


def get_market_data_from_supabase(market, target_date=None):
    """
    Get market data from Supabase for stock list display
    Fast version: Uses SQL aggregation to get latest data for all symbols
    
    Args:
        market: 'saudi' or 'us'
        target_date: Optional date filter (YYYY-MM-DD)
    
    Returns:
        JSON response with data list
    """
    from supabase_client import get_supabase_client
    
    client = get_supabase_client()
    if client is None:
        raise Exception("Supabase client not available")
    
    # Load symbol names for Saudi market
    symbols_map = {}
    if market == 'saudi':
        try:
            symbols_path = os.path.join(BASE_DIR, 'symbols_sa.txt')
            if os.path.exists(symbols_path):
                df_sym = pd.read_csv(symbols_path)
                for _, row in df_sym.iterrows():
                    symbols_map[str(row['Symbol']).strip()] = str(row['NameAr']).strip()
        except: pass
    
    # Use efficient Python aggregation - fetch all data once, process in memory
    return get_market_data_from_supabase_fallback(client, market, target_date, symbols_map)


def get_market_data_from_supabase_fallback(client, market, target_date, symbols_map):
    """
    Efficient method: Fetch recent data for all symbols at once
    Only gets last few dates (not all history) to minimize data transfer
    """
    
    # Strategy: Get recent dates for the market, then process
    # This is much faster than querying each symbol separately
    
    # First, determine what dates to fetch
    if target_date:
        # Get data up to target date
        date_query = client.table('stock_data')\
            .select('date')\
            .eq('market', market)\
            .lte('date', target_date)\
            .order('date', desc=True)\
            .limit(500)
    else:
        # Get recent dates
        date_query = client.table('stock_data')\
            .select('date')\
            .eq('market', market)\
            .order('date', desc=True)\
            .limit(500)
    
    date_result = date_query.execute()
    
    if not date_result.data or len(date_result.data) == 0:
        return jsonify({'data': [], 'date': None, 'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª'})
    
    # Get unique dates and take last 5 (enough for latest + previous)
    unique_dates = sorted(list(set([r['date'] for r in date_result.data])), reverse=True)[:5]
    
    if not unique_dates:
        return jsonify({'data': [], 'date': None, 'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØ§Ø±ÙŠØ®'})
    
    latest_date = unique_dates[0]
    
    # Fetch data for these dates only (much smaller dataset)
    data_query = client.table('stock_data')\
        .select('symbol, date, close, volume')\
        .eq('market', market)\
        .in_('date', unique_dates)\
        .order('symbol')\
        .order('date', desc=True)
    
    result = data_query.execute()
    
    if not result.data:
        return jsonify({'data': [], 'date': None, 'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª'})
    
    # Process data efficiently
    df = pd.DataFrame(result.data)
    df['date'] = pd.to_datetime(df['date'])
    
    data_list = []
    
    for symbol in df['symbol'].unique():
        symbol_df = df[df['symbol'] == symbol].sort_values('date', ascending=False)
        
        if len(symbol_df) < 1:
            continue
        
        # Latest data
        last_row = symbol_df.iloc[0]
        # Previous data
        prev_row = symbol_df.iloc[1] if len(symbol_df) > 1 else last_row
        
        close = float(last_row['close'])
        prev_close = float(prev_row['close'])
        volume = int(last_row['volume'])
        
        change = close - prev_close
        change_pct = (change / prev_close) * 100 if prev_close != 0 else 0
        
        # Get symbol name
        name = symbol
        if market == 'saudi':
            clean_sym = symbol.replace('.SR', '')
            name = symbols_map.get(clean_sym, symbol)
        
        data_list.append({
            'symbol': symbol,
            'name': name,
            'price': round(close, 2),
            'change': round(change, 2),
            'change_percent': round(change_pct, 2),
            'volume': volume
        })
    
    return jsonify({
        'data': data_list,
        'date': latest_date,
        'count': len(data_list)
    })


def get_stock_data_from_source(symbol, market):
    """
    Get stock data from Supabase or CSV (fallback)
    
    Args:
        symbol: Stock symbol
        market: 'saudi' or 'us'
    
    Returns:
        pandas DataFrame with columns: Date, Open, High, Low, Close, Volume
    """
    if USE_SUPABASE:
        try:
            # Get data from Supabase
            data = get_stock_data(symbol, market)
            if data and len(data) > 0:
                df = pd.DataFrame(data)
                df['Date'] = pd.to_datetime(df['date'])
                df = df.rename(columns={
                    'open': 'Open',
                    'high': 'High',
                    'low': 'Low',
                    'close': 'Close',
                    'volume': 'Volume'
                })
                df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]
                df = df.sort_values('Date')
                return df
        except Exception as e:
            print(f"Supabase error for {symbol}, falling back to CSV: {e}")
    
    # Fallback to CSV
    if market == 'saudi':
        directory = os.path.join(BASE_DIR, 'data_sa')
    else:
        directory = os.path.join(BASE_DIR, 'data_us')
    
    file_path = os.path.join(directory, f'{symbol}.csv')
    
    if os.path.exists(file_path):
        df = pd.read_csv(file_path)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
        return df
    
    return None


@app.route('/api/scan/weekly/<market>', methods=['GET'])
def weekly_scan(market):
    """
    ÙØ­Øµ Ø£Ø³Ø¨ÙˆØ¹ÙŠ Ù„Ù„Ø£Ø³Ù‡Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø±ÙˆØ· Ù…Ø­Ø¯Ø¯Ø©:
    1. Ø´Ù…Ø¹Ø© Ø®Ø¶Ø±Ø§Ø¡ Ø¨Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰
    2. Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ù…ØªØ¬Ø§ÙˆØ² Ø£Ùˆ Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ù‚Ù…Ø© Ø³Ø§Ø¨Ù‚Ø© (6 Ø£Ø´Ù‡Ø±)
    3. Ø§Ù„Ø­Ø¬Ù… Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ø´Ù…Ø¹Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    """
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚
        if market not in ['saudi', 'us']:
            return jsonify({'error': 'Invalid market'}), 400
        
        results = []
        total_stocks = 0
        passed_green = 0
        passed_shadow = 0
        passed_peak = 0
        passed_volume = 0
        
        # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø³Ù‡Ù…
        use_db = USE_SUPABASE  # Local variable for this request
        
        if use_db:
            try:
                symbols = get_all_symbols(market)
                print(f"Using Supabase: {len(symbols)} symbols from {market}")
            except Exception as e:
                print(f"Supabase error, falling back to CSV: {e}")
                use_db = False
        
        if not use_db:
            # Fallback to CSV
            directory = os.path.join(BASE_DIR, 'data_sa' if market == 'saudi' else 'data_us')
            if not os.path.exists(directory):
                return jsonify({'error': f'Directory not found: {directory}'}), 404
            symbols = [f[:-4] for f in os.listdir(directory) if f.endswith('.csv')]
        
        # ÙØ­Øµ ÙƒÙ„ Ø³Ù‡Ù…
        for symbol in symbols:
            total_stocks += 1
            
            try:
                # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù…Ù† Supabase Ø£Ùˆ CSV
                df = get_stock_data_from_source(symbol, market)
                
                if df is None or len(df) < 30:  # Ù†Ø­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
                    continue
                
                # ØªØ­ÙˆÙŠÙ„ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© (Date already converted in helper function)
                df.set_index('Date', inplace=True)
                weekly = df.resample('W').agg({
                    'Open': 'first',
                    'High': 'max',
                    'Low': 'min',
                    'Close': 'last',
                    'Volume': 'sum'
                }).dropna()
                
                if len(weekly) < 26:  # Ù†Ø­ØªØ§Ø¬ 6 Ø£Ø´Ù‡Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ (~26 Ø£Ø³Ø¨ÙˆØ¹)
                    continue
                
                # Ø¢Ø®Ø± Ø´Ù…Ø¹Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ©
                last_candle = weekly.iloc[-1]
                prev_candle = weekly.iloc[-2]
                prev_prev_candle = weekly.iloc[-3]  # Ø§Ù„Ø´Ù…Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
                
                # Ø§Ù„Ø´Ø±Ø· 1: Ø´Ù…Ø¹Ø© Ø®Ø¶Ø±Ø§Ø¡ Ø¨Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰
                is_green = last_candle['Close'] > last_candle['Open']
                
                if not is_green:
                    continue
                
                passed_green += 1
                
                body_size = abs(last_candle['Close'] - last_candle['Open'])
                upper_shadow = last_candle['High'] - max(last_candle['Open'], last_candle['Close'])
                
                # Ø§Ù„Ø¸Ù„ Ø§Ù„Ø¹Ù„ÙˆÙŠ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø£Ù‚Ù„ Ù…Ù† 30% Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø¬Ø³Ù…
                # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø³Ù… ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹
                if body_size > 0:
                    has_short_upper_shadow = upper_shadow < (body_size * 0.3)
                else:
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø³Ù… = 0 (doji)ØŒ Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¸Ù„ ÙÙ‚Ø·
                    has_short_upper_shadow = upper_shadow < 0.01
                
                if not has_short_upper_shadow:
                    continue
                
                passed_shadow += 1
                
                # Ø§Ù„Ø´Ø±Ø· 2: Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ù…ØªØ¬Ø§ÙˆØ² Ø£Ùˆ Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ù‚Ù…Ø© Ø³Ø§Ø¨Ù‚Ø© (6 Ø£Ø´Ù‡Ø±)
                last_6_months = weekly.iloc[-26:-1]  # Ø¢Ø®Ø± 26 Ø£Ø³Ø¨ÙˆØ¹ (6 Ø£Ø´Ù‡Ø±) Ø¹Ø¯Ø§ Ø§Ù„Ø£Ø®ÙŠØ±
                highest_in_6months = last_6_months['High'].max()
                
                # Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† >= 98% Ù…Ù† Ø£Ø¹Ù„Ù‰ Ù‚Ù…Ø©
                close_near_or_above_peak = last_candle['Close'] >= (highest_in_6months * 0.98)
                
                if not close_near_or_above_peak:
                    continue
                
                passed_peak += 1
                
                # Ø§Ù„Ø´Ø±Ø· 3: Ø§Ù„Ø­Ø¬Ù… Ø£ÙƒØ¨Ø± Ù…Ù† Ø£ÙŠ Ù…Ù† Ø§Ù„Ø´Ù…Ø¹ØªÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚ØªÙŠÙ†
                volume_increased = (last_candle['Volume'] > prev_candle['Volume']) or \
                                   (last_candle['Volume'] > prev_prev_candle['Volume'])
                
                if not volume_increased:
                    continue
                
                passed_volume += 1
                
                # Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø´Ø±ÙˆØ· ØªØ­Ù‚Ù‚Øª!
                # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø¬Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ø´Ù…Ø¹ØªÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚ØªÙŠÙ†
                max_prev_volume = max(prev_candle['Volume'], prev_prev_candle['Volume'])
                volume_ratio = (last_candle['Volume'] / max_prev_volume) if max_prev_volume > 0 else 1
                
                results.append({
                    'symbol': symbol,
                    'name': symbol,  # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ù„Ø§Ø­Ù‚Ø§Ù‹
                    'close': round(float(last_candle['Close']), 2),
                    'open': round(float(last_candle['Open']), 2),
                    'high': round(float(last_candle['High']), 2),
                    'low': round(float(last_candle['Low']), 2),
                    'volume': int(last_candle['Volume']),
                    'prev_volume': int(max_prev_volume),
                    'volume_ratio': round(float(volume_ratio), 2),
                    'highest_6m': round(float(highest_in_6months), 2),
                    'change_percent': round(((last_candle['Close'] - last_candle['Open']) / last_candle['Open']) * 100, 2),
                    'date': weekly.index[-1].strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                print(f"Error processing {symbol}: {e}")
                continue
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØºÙŠÙŠØ±
        results.sort(key=lambda x: x['change_percent'], reverse=True)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ­Øµ
        print(f"\n=== Weekly Scan Stats for {market.upper()} ===")
        print(f"Total stocks checked: {total_stocks}")
        print(f"Passed green candle: {passed_green}")
        print(f"Passed short shadow: {passed_shadow}")
        print(f"Passed peak level: {passed_peak}")
        print(f"Passed volume increase: {passed_volume}")
        print(f"Final results: {len(results)}")
        print("=" * 40)
        
        return jsonify({
            'success': True,
            'market': market,
            'count': len(results),
            'results': results,
            'stats': {
                'total_checked': total_stocks,
                'passed_green': passed_green,
                'passed_shadow': passed_shadow,
                'passed_peak': passed_peak,
                'passed_volume': passed_volume
            }
        })
        
    except Exception as e:
        print(f"Error in weekly scan: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/test/supabase', methods=['GET'])
def test_supabase():
    """Test Supabase connection and data"""
    try:
        if not USE_SUPABASE:
            return jsonify({
                'supabase_enabled': False,
                'message': 'Supabase is disabled, using CSV files'
            })
        
        # Test get_all_symbols
        us_symbols = get_all_symbols('us')
        saudi_symbols = get_all_symbols('saudi')
        
        # Get sample data for WELL
        well_data = get_stock_data('WELL', 'us')
        well_latest = None
        well_count = 0
        if well_data:
            well_count = len(well_data)
            if well_count > 0:
                well_latest = well_data[-1]['date']
        
        # Get sample data for last symbol
        last_us = sorted(us_symbols)[-1] if us_symbols else None
        last_data = get_stock_data(last_us, 'us') if last_us else None
        last_latest = None
        last_count = 0
        if last_data:
            last_count = len(last_data)
            if last_count > 0:
                last_latest = last_data[-1]['date']
        
        return jsonify({
            'supabase_enabled': True,
            'us_symbols_count': len(us_symbols),
            'saudi_symbols_count': len(saudi_symbols),
            'us_symbols_sample': us_symbols[:10] if us_symbols else [],
            'us_last_10_symbols': sorted(us_symbols)[-10:] if us_symbols else [],
            'well_test': {
                'symbol': 'WELL',
                'record_count': well_count,
                'latest_date': well_latest
            },
            'last_symbol_test': {
                'symbol': last_us,
                'record_count': last_count,
                'latest_date': last_latest
            },
            'message': 'Supabase is working correctly!'
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'supabase_enabled': USE_SUPABASE
        }), 500


if __name__ == '__main__':
    print("=" * 50)
    print("STARTING MESHALSTOCK API SERVER")
    print("=" * 50)
    print(f"Environment: {os.getenv('FLASK_ENV', 'development')}")
    print(f"Server running at: http://{os.getenv('HOST', '0.0.0.0')}:{os.getenv('PORT', '5000')}")
    print("\nAvailable Endpoints:")
    print("  - GET  /api/health              Check health")
    print("  - GET  /api/test/supabase       Test Supabase connection")
    print("  - POST /api/auth/login          Login (get token)")
    print("  - POST /api/auth/verify         Verify token")
    print("  - POST /api/fetch/saudi         Fetch Saudi data [AUTH]")
    print("  - POST /api/fetch/us            Fetch US data [AUTH]")
    print("  - GET  /api/market-summary      Market summary")
    print("  - GET  /api/status/<job_id>     Job status [AUTH]")
    print("  - GET  /api/jobs                List jobs [AUTH]")
    print("  - GET  /api/symbols/<market>    Get symbols")
    print("  - GET  /api/history/<market>    Get history")
    print("  - GET  /api/scan/fibo_gann      Scan opportunities")
    print("  - GET  /api/scan/weekly         Weekly scan")
    print("  - GET  /api/market-data         Market data")
    print("=" * 50)
    print(f"\nâš ï¸  SECURITY NOTES:")
    print(f"   - Change SECRET_KEY in .env file")
    print(f"   - Change ADMIN_PASSWORD in .env file")
    print(f"   - Configure ALLOWED_ORIGINS for production")
    print("=" * 50)
    
    # Initialize data if not present (background fetch)
    try:
        from initialize_data import initialize_data
        print("\nğŸ” Checking for data files...")
        initialize_data(background=True)
    except Exception as e:
        print(f"âš ï¸  Could not initialize data: {e}")
        print("You can manually update data from the web interface.")
    
    print("\n" + "=" * 50)
    print("ğŸš€ SERVER READY")
    print("=" * 50)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„
    debug_mode = os.getenv('FLASK_DEBUG', 'False') == 'True'
    host = os.getenv('HOST', '0.0.0.0')
    port = int(os.getenv('PORT', '5000'))
    
    app.run(debug=debug_mode, use_reloader=debug_mode, host=host, port=port, threaded=True)
